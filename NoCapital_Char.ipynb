{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NoCapital_Char.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4AKuYXZGDESEnj2/bwzoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Inzk652HZ7xQ"},"source":["This notebook is to a train sequence to sequence model for punctuation prediction. This project followed the Neural Machine Translation model architecture which is available from TensorFlow.\r\n","https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt#data_cleaning_and_data_preparation \\\r\n","\\\r\n","***For this notebook, character tokenization is used and the input dataset to the decoder is also changed to small case. ***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLQ4uuw_uW-L","executionInfo":{"status":"ok","timestamp":1611484951874,"user_tz":-480,"elapsed":17324,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"4d8bb9d7-eb5a-4663-c374-a09222884b9f"},"source":["# from google.colab import drive\r\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zPaCjYWCXLcG"},"source":["**Import necessary libraries** \r\n","\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"Ier3HUGYZmrb"},"source":["from google.colab import files\r\n","\r\n","import tensorflow as tf\r\n","import tensorflow_addons as tfa\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import matplotlib.ticker as ticker\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","from nltk.translate.bleu_score import corpus_bleu\r\n","\r\n","import re\r\n","import numpy as np\r\n","import os\r\n","import io\r\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UZbhrF-bW-6"},"source":["We have two corpora with the same sentences but different preprocessing, which are normalized_corpus.txt used for input to the encoder and 2008.txt used for decoder and it follows the format of our output thus called it target language. \\\r\n","In order to use the data for training, the data must be preprocessed, tokenized and converted to sequences of integers using the following preprocessing methods.\r\n","\r\n","**Preprocess the sentences in corpus** \\\r\n","For the normalized_corpus.txt corpus, the sentences are preprocessed beforehand thus no need to be preprocessed here. \\\r\n","For the 2008.txt corpus, the punctuation in each sentence is added a space before it by using regular expression for the tokenization later.\r\n","For both the corpora, we add <start> and <end> token for each sentence so that the model know when to start and stop predicting. \r\n","\r\n","**Tokenize** \\\r\n","For a neural network to predict on text data, it first has to be turned into data it can understand. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s). \\\r\n","We use Keras's Tokenizer function to tokenize each unique character/word into a number.\r\n","\r\n","\r\n","**Padding** \\\r\n","When batching the sequence of number together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length. \\\r\n","Make sure all the input sequences have the same length and all the target sequences have the same length by adding padding to the end of each sequence using 'post' in Keras's pad_sequences function."]},{"cell_type":"code","metadata":{"id":"fHKW3nA0anqV"},"source":["class NMTDataset:\r\n","    def __init__(self):\r\n","        self.inp_lang_tokenizer = None\r\n","        self.targ_lang_tokenizer = None\r\n","\r\n","    ## Step 1 and Step 2 \r\n","    def preprocess_sentence(self, w, type):\r\n","\r\n","        # creating a space between a word and the punctuation following it\r\n","        # eg: \"he is a boy.\" => \"he is a boy .\"\r\n","        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\r\n","\r\n","        if type == \"t\":\r\n","            w = re.sub(r\"([\\.])$\", r\" \\1 \", w) # for . at the end\r\n","            w = re.sub(\"([^a-zA-Z0-9_\\.-])\", r\" \\1 \", w)  #except for - and . which is kata ganda\r\n","        w = re.sub('[\\s]+', \" \" , w) #remove excessive spaces\r\n","\r\n","        w = w.strip() #for \" <end>\"\r\n","        \r\n","        # adding a start and an end token to the sentence\r\n","        # so that the model know when to start and stop predicting.\r\n","        w = '^ ' + w + ' |'\r\n","        return w\r\n","\r\n","    def create_dataset(self, num_examples):\r\n","        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\r\n","        with open('/content/drive/My Drive/files/normalized_corpus.txt' , 'r', encoding='windows-1256') as file1:\r\n","            input_lang = file1.readlines()\r\n","            input_lang = [self.preprocess_sentence(w,\"i\") for w in input_lang[:num_examples]]\r\n","            #input_lang = [self.preprocess_sentence(w,\"i\") for w in input_lang]\r\n","        with open('/content/drive/My Drive/files/2008.txt' , 'r', encoding='windows-1256') as file2:\r\n","            target_lang = file2.readlines()\r\n","            target_lang = [self.preprocess_sentence(w,\"t\") for w in target_lang[:num_examples]]\r\n","            #target_lang = [self.preprocess_sentence(w,\"t\") for w in target_lang]\r\n","        return target_lang, input_lang\r\n","\r\n","    # Step 3 and Step 4\r\n","    def tokenize(self, lang): \r\n","        # lang = list of sentences in a language\r\n","\r\n","        # if type == \"t\":\r\n","        #   lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower = False, char_level=True, oov_token='<OOV>')\r\n","        # else:\r\n","        #   lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower = True, char_level=True, oov_token='<OOV>')\r\n","        \r\n","        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True, oov_token='<OOV>')\r\n","        lang_tokenizer.fit_on_texts(lang)\r\n","\r\n","        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \r\n","        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\r\n","        tensor = lang_tokenizer.texts_to_sequences(lang) \r\n","       \r\n","        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \r\n","        ## and pads the sequences to match the longest sequences in the given input\r\n","        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\r\n","\r\n","        # determine maximum length output sequence\r\n","        target_max_len = max(len(s) for s in tensor)\r\n","        print('Max Target Length: ', target_max_len)\r\n","        print('VOCAB Size: ', len(lang_tokenizer.word_index))\r\n","        print(lang[283])\r\n","        print(tensor[283])\r\n","        return tensor, lang_tokenizer\r\n","\r\n","    def load_dataset(self, num_examples=None):\r\n","        # creating cleaned input, output pairs\r\n","        targ_lang, inp_lang = self.create_dataset(num_examples)\r\n","\r\n","        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\r\n","        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\r\n","\r\n","        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\r\n","\r\n","    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\r\n","        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(num_examples)\r\n","\r\n","        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.3)\r\n","\r\n","        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\r\n","        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n","\r\n","        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\r\n","        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\r\n","\r\n","        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kmhB78PY4Mt"},"source":["**Setting some parameter**:\r\n","\r\n","\r\n","1.   BUFFER_SIZE: to shuffle the dataset\r\n","2.   BATCH_SIZE:  the number of samples that will be propagated through the network. It is set to 32 because 64 will cause issue of out of memory\r\n","3.   NUM_EXAMPLES: number of lines in the corpus to train the networ. Only half of the corpus is used for faster training.\r\n","\r\n","Maximum sentence length, vocabulary size for both datasets and examples of sequences of integers after tokenizing and converting are printed out for debugging.\r\n"]},{"cell_type":"code","metadata":{"id":"1O0gVPBBakn0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611485042228,"user_tz":-480,"elapsed":54112,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"063d6e37-c443-4cd3-e5a5-a3b203f5b74c"},"source":["BUFFER_SIZE = 32000\r\n","BATCH_SIZE = 32\r\n","#MAX_VOCAB_SIZE = 20000\r\n","# Let's limit the training examples for faster training\r\n","num_examples = 250000\r\n","\r\n","dataset_creator = NMTDataset()\r\n","train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max Target Length:  930\n","VOCAB Size:  74\n","^ dalam kejadian kira-kira pukul 11.30 malam itu rakan muhammad firdhaus harun setapa 19 hanya cedera ringan dan menerima rawatan sebagai pesakit luar di hospital berdekatan |\n","[22  3 13  2 15  2 11  3  7  5 21  2 13  6  2  4  3  7  6  8  2 26  7  6\n","  8  2  3 16  9  7  9 15  3 29 29 32 34 28  3 11  2 15  2 11  3  6 10  9\n","  3  8  2  7  2  4  3 11  9 18  2 11 11  2 13  3 27  6  8 13 18  2  9 12\n","  3 18  2  8  9  4  3 12  5 10  2 16  2  3 29 36  3 18  2  4 19  2  3 24\n","  5 13  5  8  2  3  8  6  4 14  2  4  3 13  2  4  3 11  5  4  5  8  6 11\n","  2  3  8  2 25  2 10  2  4  3 12  5 17  2 14  2  6  3 16  5 12  2  7  6\n"," 10  3 15  9  2  8  3 13  6  3 18 20 12 16  6 10  2 15  3 17  5  8 13  5\n","  7  2 10  2  4  3 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n","Max Target Length:  993\n","VOCAB Size:  78\n","^ Dalam kejadian kira-kira pukul 11.30 malam itu , rakan Muhammad Firdhaus , Harun Setapa , 19 , hanya cedera ringan dan menerima rawatan sebagai pesakit luar di hospital berdekatan . |\n","[23  3 13  2 15  2 11  3  7  5 21  2 13  6  2  4  3  7  6  8  2 28  7  6\n","  8  2  3 16  9  7  9 15  3 31 31 22 37 30  3 11  2 15  2 11  3  6 10  9\n","  3 25  3  8  2  7  2  4  3 11  9 18  2 11 11  2 13  3 29  6  8 13 18  2\n","  9 12  3 25  3 18  2  8  9  4  3 12  5 10  2 16  2  3 25  3 31 40  3 25\n","  3 18  2  4 19  2  3 26  5 13  5  8  2  3  8  6  4 14  2  4  3 13  2  4\n","  3 11  5  4  5  8  6 11  2  3  8  2 27  2 10  2  4  3 12  5 17  2 14  2\n","  6  3 16  5 12  2  7  6 10  3 15  9  2  8  3 13  6  3 18 20 12 16  6 10\n","  2 15  3 17  5  8 13  5  7  2 10  2  4  3 22  3 24  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED4uGRU2JM67","executionInfo":{"status":"ok","timestamp":1611485042958,"user_tz":-480,"elapsed":52812,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"0c572390-bd0e-428d-f311-afa692b7d459"},"source":["example_input_batch, example_target_batch = next(iter(train_dataset))\r\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([32, 930]), TensorShape([32, 993]))"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"KaDcWaW4j6WJ"},"source":["**Setting parameters** \\\r\n","Both the embedding dimension and network size is set to 256 to reduce the trainable parameters for faster training."]},{"cell_type":"code","metadata":{"id":"VZwldcgpMWZD"},"source":["vocab_inp_size = len(inp_lang.word_index)+1\r\n","vocab_tar_size = len(targ_lang.word_index)+1\r\n","max_length_input = example_input_batch.shape[1]\r\n","max_length_output = example_target_batch.shape[1]\r\n","\r\n","embedding_dim = 256\r\n","units = 256\r\n","steps_per_epoch = num_examples//BATCH_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9m3GztAarrJ","executionInfo":{"status":"ok","timestamp":1611485042961,"user_tz":-480,"elapsed":50087,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"13e1d5f1-4a3e-467a-d451-9b7541a112e5"},"source":["print(\"max_length_input, max_length_output, vocab_size_input, vocab_size_output\")\r\n","max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"],"execution_count":null,"outputs":[{"output_type":"stream","text":["max_length_input, max_length_output, vocab_size_input, vocab_size_output\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(930, 993, 75, 79)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"_yNFlec7kwxr"},"source":["**Encoder and decoder structure** \\\r\n","In this part of the code, we used the same technique of Neural Machine Translation provided by TensorFlow. \\\r\n","Embedding is used to capture more precise syntactic and semantic word relationships. \\\r\n","LSTM layer is used in the encoder. \\\r\n","For decoder, luong attention mechanism is used to selectively concentrate on a few relevant things in encoder, while ignoring others in deep neural networks"]},{"cell_type":"code","metadata":{"id":"4O1B0l89a3dP"},"source":["##### \r\n","\r\n","class Encoder(tf.keras.Model):\r\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n","    super(Encoder, self).__init__()\r\n","    self.batch_sz = batch_sz\r\n","    self.enc_units = enc_units\r\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n","\r\n","    ##________ LSTM layer in Encoder ------- ##\r\n","    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\r\n","                                   return_sequences=True,\r\n","                                   return_state=True,\r\n","                                   recurrent_initializer='glorot_uniform')\r\n","\r\n","\r\n","\r\n","  def call(self, x, hidden):\r\n","    x = self.embedding(x)\r\n","    output, h, c = self.lstm_layer(x, initial_state = hidden)\r\n","    return output, h, c\r\n","\r\n","  def initialize_hidden_state(self):\r\n","    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcXDiD3XbDks","executionInfo":{"status":"ok","timestamp":1611485045045,"user_tz":-480,"elapsed":48152,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"e277167b-9335-447b-e108-5aa3523eda91"},"source":["## Test Encoder Stack\r\n","\r\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\r\n","\r\n","\r\n","# sample input\r\n","sample_hidden = encoder.initialize_hidden_state()\r\n","sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\r\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\r\n","print ('Encoder h vector shape: (batch size, units) {}'.format(sample_h.shape))\r\n","print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (32, 930, 256)\n","Encoder h vector shape: (batch size, units) (32, 256)\n","Encoder c vector shape: (batch size, units) (32, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9iL5S_QsbHH2"},"source":["class Decoder(tf.keras.Model):\r\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\r\n","    super(Decoder, self).__init__()\r\n","    self.batch_sz = batch_sz\r\n","    self.dec_units = dec_units\r\n","    self.attention_type = attention_type\r\n","\r\n","    # Embedding Layer\r\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n","\r\n","    #Final Dense layer on which softmax will be applied\r\n","    self.fc = tf.keras.layers.Dense(vocab_size)\r\n","\r\n","    # Define the fundamental cell for decoder recurrent structure\r\n","    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\r\n","\r\n","\r\n","\r\n","    # Sampler\r\n","    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\r\n","\r\n","    # Create attention mechanism with memory = None\r\n","    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \r\n","                                                              None, self.batch_sz*[max_length_input], self.attention_type)\r\n","\r\n","    # Wrap attention mechanism with the fundamental rnn cell of decoder\r\n","    self.rnn_cell = self.build_rnn_cell(batch_sz)\r\n","\r\n","    # Define the decoder with respect to fundamental rnn cell\r\n","    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\r\n","\r\n","\r\n","  def build_rnn_cell(self, batch_sz):\r\n","    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \r\n","                                  self.attention_mechanism, attention_layer_size=self.dec_units)\r\n","    return rnn_cell\r\n","\r\n","  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\r\n","    # ------------- #\r\n","    # typ: Which sort of attention (Bahdanau, Luong)\r\n","    # dec_units: final dimension of attention outputs \r\n","    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\r\n","    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\r\n","\r\n","    if(attention_type=='bahdanau'):\r\n","      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n","    else:\r\n","      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n","\r\n","  def build_initial_state(self, batch_sz, encoder_state, Dtype):\r\n","    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\r\n","    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\r\n","    return decoder_initial_state\r\n","\r\n","\r\n","  def call(self, inputs, initial_state):\r\n","    x = self.embedding(inputs)\r\n","    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\r\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUxe57IsbJy0","executionInfo":{"status":"ok","timestamp":1611485056954,"user_tz":-480,"elapsed":56684,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"7d98f452-b3a7-4d40-c57f-0505aefe0533"},"source":["# Test decoder stack\r\n","\r\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\r\n","sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\r\n","decoder.attention_mechanism.setup_memory(sample_output)\r\n","initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\r\n","\r\n","\r\n","sample_decoder_outputs = decoder(sample_x, initial_state)\r\n","\r\n","print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Decoder Outputs Shape:  (32, 992, 79)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G1Q8_t4LlDDG"},"source":["**Optimizers and loss function** \\\r\n","Adam is used as the optimizer.\r\n"]},{"cell_type":"code","metadata":{"id":"wItxuGyebR_0"},"source":["optimizer = tf.keras.optimizers.Adam()\r\n","\r\n","\r\n","def loss_function(real, pred):\r\n","  # real shape = (BATCH_SIZE, max_length_output)\r\n","  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\r\n","  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n","  loss = cross_entropy(y_true=real, y_pred=pred)\r\n","  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\r\n","  mask = tf.cast(mask, dtype=loss.dtype)  \r\n","  loss = mask* loss\r\n","  loss = tf.reduce_mean(loss)\r\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2g-vfQQJln9C"},"source":["**Define checkpoint directory**"]},{"cell_type":"code","metadata":{"id":"m1EC6NW-bb6Y"},"source":["checkpoint_dir = './training_checkpoints'\r\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n","                                 encoder=encoder,\r\n","                                 decoder=decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uiZwyDm2mjHl"},"source":["**One train step operations** \\\r\n","This function carries out the training of a batch of the data:\r\n","\r\n","\r\n","1.   Call the encoder for the batch input sequence — the output is the encoded vector\r\n","2.   Set the decoder initial states to the encoded vector\r\n","3.   Call the decoder, taking the right-shifted target sequence as the input\r\n","4.   Calculate the loss and accuracy of the batch data\r\n","5.   Update the learnable parameters of the encoder and the decoder\r\n","6.   Update the optimizer\r\n"]},{"cell_type":"code","metadata":{"id":"YNRnHGSsbeJc"},"source":["@tf.function\r\n","def train_step(inp, targ, enc_hidden):\r\n","  loss = 0\r\n","\r\n","  with tf.GradientTape() as tape:\r\n","    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\r\n","\r\n","\r\n","    dec_input = targ[ : , :-1 ] # Ignore <end> token\r\n","    real = targ[ : , 1: ]         # ignore <start> token\r\n","\r\n","    # Set the AttentionMechanism object with encoder_outputs\r\n","    decoder.attention_mechanism.setup_memory(enc_output)\r\n","\r\n","    # Create AttentionWrapperState as initial_state for decoder\r\n","    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\r\n","    pred = decoder(dec_input, decoder_initial_state)\r\n","    logits = pred.rnn_output\r\n","    loss = loss_function(real, logits)\r\n","\r\n","  variables = encoder.trainable_variables + decoder.trainable_variables\r\n","  gradients = tape.gradient(loss, variables)\r\n","  optimizer.apply_gradients(zip(gradients, variables))\r\n","\r\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVOuNmfBl5aY"},"source":["**Training** \\\r\n","Epoch is set to 2 to avoid hitting the gpu usage limit in the Google Colab."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdFFzyBwbgVU","executionInfo":{"status":"ok","timestamp":1611450046358,"user_tz":-480,"elapsed":31629660,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"7de18f3b-04c4-4af8-bcaa-3e22d7075296"},"source":["EPOCHS = 2\r\n","\r\n","for epoch in range(EPOCHS):\r\n","  start = time.time()\r\n","\r\n","  enc_hidden = encoder.initialize_hidden_state()\r\n","  total_loss = 0\r\n","  # print(enc_hidden[0].shape, enc_hidden[1].shape)\r\n","\r\n","  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\r\n","    batch_loss = train_step(inp, targ, enc_hidden)\r\n","    total_loss += batch_loss\r\n","\r\n","    if batch % 100 == 0:\r\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n","                                                   batch,\r\n","                                                   batch_loss.numpy()))\r\n","  # saving (checkpoint) the model every 2 epochs\r\n","  # if (epoch + 1) % 2 == 0:\r\n","  #   checkpoint.save(file_prefix = checkpoint_prefix)\r\n","  checkpoint.save(file_prefix = checkpoint_prefix)\r\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\r\n","                                      total_loss / steps_per_epoch))\r\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 0.6149\n","Epoch 1 Batch 100 Loss 0.3084\n","Epoch 1 Batch 200 Loss 0.2547\n","Epoch 1 Batch 300 Loss 0.2437\n","Epoch 1 Batch 400 Loss 0.2710\n","Epoch 1 Batch 500 Loss 0.2048\n","Epoch 1 Batch 600 Loss 0.0563\n","Epoch 1 Batch 700 Loss 0.0117\n","Epoch 1 Batch 800 Loss 0.0117\n","Epoch 1 Batch 900 Loss 0.0126\n","Epoch 1 Batch 1000 Loss 0.0109\n","Epoch 1 Batch 1100 Loss 0.0098\n","Epoch 1 Batch 1200 Loss 0.0084\n","Epoch 1 Batch 1300 Loss 0.0091\n","Epoch 1 Batch 1400 Loss 0.0213\n","Epoch 1 Batch 1500 Loss 0.0127\n","Epoch 1 Batch 1600 Loss 0.0072\n","Epoch 1 Batch 1700 Loss 0.0112\n","Epoch 1 Batch 1800 Loss 0.0091\n","Epoch 1 Batch 1900 Loss 0.0077\n","Epoch 1 Batch 2000 Loss 0.0086\n","Epoch 1 Batch 2100 Loss 0.0063\n","Epoch 1 Batch 2200 Loss 0.0083\n","Epoch 1 Batch 2300 Loss 0.0089\n","Epoch 1 Batch 2400 Loss 0.0066\n","Epoch 1 Batch 2500 Loss 0.0087\n","Epoch 1 Batch 2600 Loss 0.0088\n","Epoch 1 Batch 2700 Loss 0.0069\n","Epoch 1 Batch 2800 Loss 0.0079\n","Epoch 1 Batch 2900 Loss 0.0066\n","Epoch 1 Batch 3000 Loss 0.0081\n","Epoch 1 Batch 3100 Loss 0.0079\n","Epoch 1 Batch 3200 Loss 0.0120\n","Epoch 1 Batch 3300 Loss 0.0062\n","Epoch 1 Batch 3400 Loss 0.0061\n","Epoch 1 Batch 3500 Loss 0.0090\n","Epoch 1 Batch 3600 Loss 0.0087\n","Epoch 1 Batch 3700 Loss 0.0066\n","Epoch 1 Batch 3800 Loss 0.0154\n","Epoch 1 Batch 3900 Loss 0.0064\n","Epoch 1 Batch 4000 Loss 0.0070\n","Epoch 1 Batch 4100 Loss 0.0061\n","Epoch 1 Batch 4200 Loss 0.0083\n","Epoch 1 Batch 4300 Loss 0.0057\n","Epoch 1 Batch 4400 Loss 0.0102\n","Epoch 1 Batch 4500 Loss 0.2670\n","Epoch 1 Batch 4600 Loss 0.0206\n","Epoch 1 Batch 4700 Loss 0.0116\n","Epoch 1 Batch 4800 Loss 0.0110\n","Epoch 1 Batch 4900 Loss 0.0114\n","Epoch 1 Batch 5000 Loss 0.0079\n","Epoch 1 Batch 5100 Loss 0.0089\n","Epoch 1 Batch 5200 Loss 0.0071\n","Epoch 1 Batch 5300 Loss 0.0066\n","Epoch 1 Batch 5400 Loss 0.0065\n","Epoch 1 Loss 0.0304\n","Time taken for 1 epoch 15782.343306303024 sec\n","\n","Epoch 2 Batch 0 Loss 0.0103\n","Epoch 2 Batch 100 Loss 0.0097\n","Epoch 2 Batch 200 Loss 0.0073\n","Epoch 2 Batch 300 Loss 0.0077\n","Epoch 2 Batch 400 Loss 0.0073\n","Epoch 2 Batch 500 Loss 0.0072\n","Epoch 2 Batch 600 Loss 0.0062\n","Epoch 2 Batch 700 Loss 0.0077\n","Epoch 2 Batch 800 Loss 0.0085\n","Epoch 2 Batch 900 Loss 0.0119\n","Epoch 2 Batch 1000 Loss 0.0067\n","Epoch 2 Batch 1100 Loss 0.2046\n","Epoch 2 Batch 1200 Loss 0.0314\n","Epoch 2 Batch 1300 Loss 0.0211\n","Epoch 2 Batch 1400 Loss 0.0131\n","Epoch 2 Batch 1500 Loss 0.0126\n","Epoch 2 Batch 1600 Loss 0.0143\n","Epoch 2 Batch 1700 Loss 0.0107\n","Epoch 2 Batch 1800 Loss 0.0124\n","Epoch 2 Batch 1900 Loss 0.0102\n","Epoch 2 Batch 2000 Loss 0.0089\n","Epoch 2 Batch 2100 Loss 0.0084\n","Epoch 2 Batch 2200 Loss 0.0073\n","Epoch 2 Batch 2300 Loss 0.0078\n","Epoch 2 Batch 2400 Loss 0.0076\n","Epoch 2 Batch 2500 Loss 0.0092\n","Epoch 2 Batch 2600 Loss 0.0102\n","Epoch 2 Batch 2700 Loss 0.0078\n","Epoch 2 Batch 2800 Loss 0.0096\n","Epoch 2 Batch 2900 Loss 0.0081\n","Epoch 2 Batch 3000 Loss 0.0095\n","Epoch 2 Batch 3100 Loss 0.0073\n","Epoch 2 Batch 3200 Loss 0.0055\n","Epoch 2 Batch 3300 Loss 0.0088\n","Epoch 2 Batch 3400 Loss 0.0077\n","Epoch 2 Batch 3500 Loss 0.0077\n","Epoch 2 Batch 3600 Loss 0.0066\n","Epoch 2 Batch 3700 Loss 0.0065\n","Epoch 2 Batch 3800 Loss 0.0061\n","Epoch 2 Batch 3900 Loss 0.0078\n","Epoch 2 Batch 4000 Loss 0.0074\n","Epoch 2 Batch 4100 Loss 0.0075\n","Epoch 2 Batch 4200 Loss 0.0500\n","Epoch 2 Batch 4300 Loss 0.0106\n","Epoch 2 Batch 4400 Loss 0.0259\n","Epoch 2 Batch 4500 Loss 0.0114\n","Epoch 2 Batch 4600 Loss 0.0103\n","Epoch 2 Batch 4700 Loss 0.0085\n","Epoch 2 Batch 4800 Loss 0.0083\n","Epoch 2 Batch 4900 Loss 0.0083\n","Epoch 2 Batch 5000 Loss 0.0091\n","Epoch 2 Batch 5100 Loss 0.0061\n","Epoch 2 Batch 5200 Loss 0.0055\n","Epoch 2 Batch 5300 Loss 0.0080\n","Epoch 2 Batch 5400 Loss 0.0049\n","Epoch 2 Loss 0.0107\n","Time taken for 1 epoch 15794.715160608292 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xyClSLkmJO0J"},"source":["# !zip -r './training_checkpoints.zip' './training_checkpoints'\r\n","\r\n","# files.download(\"./training_checkpoints.zip\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_Xb8s5ImwWz"},"source":["**Evaluate sentence operations** \\\r\n","The input sentence is preprocessed, tokenized and mapped to pretrained vocabulary's word index. It then passed to the model for predicting the output. tf-addons BasicDecoder is used for decoding."]},{"cell_type":"code","metadata":{"id":"xWApH1E7cCTP"},"source":["def evaluate_sentence(sentence):\r\n","  sentence = dataset_creator.preprocess_sentence(sentence, \"i\")\r\n","\r\n","  inputs = []\r\n","  #map oov token to 1\r\n","  # for i in sentence.split(' '):\r\n","  #   try:\r\n","  #     inputs.append(inp_lang.word_index[i])\r\n","  #   except:\r\n","  #     inputs.append(1)\r\n","\r\n","  for i, v in enumerate(sentence): \r\n","    inputs.append(inp_lang.word_index[v])\r\n","  \r\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n","                                                          maxlen=max_length_input,\r\n","                                                          padding='post')\r\n","  inputs = tf.convert_to_tensor(inputs)\r\n","  inference_batch_size = inputs.shape[0]\r\n","  result = ''\r\n","\r\n","  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n","  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n","\r\n","  dec_h = enc_h\r\n","  dec_c = enc_c\r\n","\r\n","  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['^'])\r\n","  end_token = targ_lang.word_index['|']\r\n","\r\n","  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\r\n","\r\n","  # Instantiate BasicDecoder object\r\n","  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\r\n","  # Setup Memory in decoder stack\r\n","  decoder.attention_mechanism.setup_memory(enc_out)\r\n","\r\n","  # set decoder_initial_state\r\n","  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\r\n","\r\n","\r\n","  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \r\n","  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \r\n","  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\r\n","\r\n","  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n","\r\n","  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\r\n","  \r\n","  return outputs.sample_id.numpy()\r\n","\r\n","def translate1(sentence,actual):\r\n","  result = evaluate_sentence(sentence)\r\n","  print(result)\r\n","  result = targ_lang.sequences_to_texts(result)\r\n","  print('Input: %s' % (sentence))\r\n","  print('Actual: %s' % (actual))\r\n","  print('Predicted translation: {}'.format(result))\r\n","\r\n","def translate2(sentence):\r\n","  result = evaluate_sentence(sentence)\r\n","  result = targ_lang.sequences_to_texts(result)\r\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2uFC1nanRTZ"},"source":["**Restoring the checkpoint**"]},{"cell_type":"code","metadata":{"id":"R6E1d8aCageR"},"source":["# restoring the latest checkpoint in checkpoint_dir\r\n","#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tvSqCVe91Dk0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611485058813,"user_tz":-480,"elapsed":34073,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"14c3bc40-26ba-45d4-86a0-eb4b04fbb07b"},"source":["# !unzip -u \"/content/drive/My Drive/training_checkpoints.zip\" -d \"/content\"    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/training_checkpoints.zip\n","   creating: /content/training_checkpoints/\n","  inflating: /content/training_checkpoints/checkpoint  \n","  inflating: /content/training_checkpoints/ckpt-1.index  \n","  inflating: /content/training_checkpoints/ckpt-2.index  \n","  inflating: /content/training_checkpoints/ckpt-1.data-00000-of-00001  \n","  inflating: /content/training_checkpoints/ckpt-2.data-00000-of-00001  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r_wonV8UPdE2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611485058815,"user_tz":-480,"elapsed":30779,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"6d3e617e-3b95-4d50-bddd-6d34fa436188"},"source":["checkpoint.restore(tf.train.latest_checkpoint(\"/content/training_checkpoints\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f54d9e8b3c8>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"h-kRfz89mPkL"},"source":["**Testing the model** \\\r\n","Here are examples from testing dataset that input to the model and the predictions are as below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koX5s7yhgB_p","executionInfo":{"status":"ok","timestamp":1611450048545,"user_tz":-480,"elapsed":31631821,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"88dc6502-5bed-45ee-d1fd-a73258bd4bf3"},"source":["translate1(\"dalam situasi begini juga umat islam perlu memiliki sifat hiba sedih dan marah apabila melihat saudara seislam ditindas dan hak mereka diketepikan\", \"Dalam situasi begini juga, umat Islam perlu memiliki sifat hiba, sedih dan marah apabila melihat saudara seIslam ditindas dan hak mereka diketepikan.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 3 13  2 15  2 11  3 12  6 10  9  2 12  6  3 17  5 14  6  4  6  3 21  9\n","  14  2  3  9 11  2 10  3  6 12 15  2 11  3 16  5  8 15  9  3 11  5 11  6\n","  15  6  7  6  3 12  6 29  2 10  3 18  6 17  2  3 12  5 13  6 18  3 13  2\n","   4  3 11  2  8  2 18  3  2 16  2 17  6 15  2  3 11  5 15  6 18  2 10  3\n","  12  2  9 13  2  8  2  3 12  5  6 12 15  2 11  3 13  6 10  6  4 13  2 12\n","   3 13  2  4  3 18  2  7  3 11  5  8  5  7  2  3 13  6  7  5 10  5 16  6\n","   7  2  4  3 22  3 24]]\n","Input: dalam situasi begini juga umat islam perlu memiliki sifat hiba sedih dan marah apabila melihat saudara seislam ditindas dan hak mereka diketepikan\n","Actual: Dalam situasi begini juga, umat Islam perlu memiliki sifat hiba, sedih dan marah apabila melihat saudara seIslam ditindas dan hak mereka diketepikan.\n","Predicted translation: ['  d a l a m   s i t u a s i   b e g i n i   j u g a   u m a t   i s l a m   p e r l u   m e m i l i k i   s i f a t   h i b a   s e d i h   d a n   m a r a h   a p a b i l a   m e l i h a t   s a u d a r a   s e i s l a m   d i t i n d a s   d a n   h a k   m e r e k a   d i k e t e p i k a n   .   |']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gpT4RHt3Qpw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611450048889,"user_tz":-480,"elapsed":31632151,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"7ed25d70-6b5e-4fe2-a90d-8562334169c4"},"source":["translate1(\"sebagai orang islam yang beriman elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya allah s.w.t\", \"\\\"Sebagai orang Islam yang beriman, elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya Allah s.w.t. \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 3 12  5 17  2 14  2  6  3 20  8  2  4 14  3  6 12 15  2 11  3 19  2  4\n","  14  3 17  5  8  6 11  2  4  3  5 15  2  7  7  2  4  3 16  5  8  2 12  2\n","   2  4  3  8  9 14  6  3 13  2  4  3 12  6  2 28 12  6  2  3  2 10  2 12\n","   3 16  5  4 14 20  8 17  2  4  2  4  3 19  2  4 14  3 13  6 15  2  7  9\n","   7  2  4  3  7  5  8  2  4  2  3 12  5 12  9  4 14 14  9 18  4 19  2  3\n","   2 15 15  2 18  3 12 22 27 22 10 22  3 24]]\n","Input: sebagai orang islam yang beriman elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya allah s.w.t\n","Actual: \"Sebagai orang Islam yang beriman, elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya Allah s.w.t. \n","Predicted translation: ['  s e b a g a i   o r a n g   i s l a m   y a n g   b e r i m a n   e l a k k a n   p e r a s a a n   r u g i   d a n   s i a - s i a   a t a s   p e n g o r b a n a n   y a n g   d i l a k u k a n   k e r a n a   s e s u n g g u h n y a   a l l a h   s . w . t .   |']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FWEa0I3y3lGy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611463761392,"user_tz":-480,"elapsed":1836,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"a0656991-f949-4d07-dd94-4249d63a7b52"},"source":["translate1(\"dalam konteks ini kita harus memperbetulkan persepsi dalam masyarakat\", \"\\\"Dalam konteks ini, kita harus memperbetulkan persepsi dalam masyarakat. \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 3 13  2 15  2 11  3  7 20  4 10  5  7 12  3  6  4  6  3  7  6 10  2  3\n","  18  2  8  9 12  3 11  5 11 16  5  8 17  5 10  9 15  7  2  4  3 16  5  8\n","  12  5 16 12  6  3 13  2 15  2 11  3 11  2 12 19  2  8  2  7  2 10  3 22\n","   3 24]]\n","Input: dalam konteks ini kita harus memperbetulkan persepsi dalam masyarakat\n","Actual: \"Dalam konteks ini, kita harus memperbetulkan persepsi dalam masyarakat. \n","Predicted translation: ['  d a l a m   k o n t e k s   i n i   k i t a   h a r u s   m e m p e r b e t u l k a n   p e r s e p s i   d a l a m   m a s y a r a k a t   .   |']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jvZ3LY5YnqoW"},"source":["The whole dataset is passed through the model for prediction and the output are written to the prediction file."]},{"cell_type":"code","metadata":{"id":"l1zRKgrU6Mvg"},"source":["with open ('/content/drive/My Drive/files/norm.txt' , 'r', encoding='windows-1256') as test_file:\r\n","  for line in test_file.readlines():\r\n","    #there are some cases where the prediction enters an infinite loop and cause out of memory\r\n","    try:\r\n","      result = translate2(line)\r\n","      result = [i.replace(\" |\" ,\"\") for i in result]\r\n","    except:\r\n","      result = \"nil\"\r\n","    \r\n","    with open ('/content/drive/My Drive/prediction.txt' , 'a', encoding='windows-1256') as test3_file:\r\n","      test3_file.writelines(result)\r\n","      test3_file.writelines('\\n')\r\n","\r\n","\r\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSnbo05dn4sj"},"source":["**Evaluating the performance** \\\r\n","Bleu score is a score for comparing a candidate translation of text to one or more reference translations. It is suitable to evaluate text generated for a suite of natural language processing tasks."]},{"cell_type":"code","metadata":{"id":"1H5DaTIOhtFi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611485060446,"user_tz":-480,"elapsed":27032,"user":{"displayName":"tan Kaachun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY83BQ67tcCgLkKikBrKwSO-rdNvNT_Z_wBMSgig=s64","userId":"02956887674133276458"}},"outputId":"391ddd2b-b8d8-442c-e787-1fd536b75c11"},"source":["actual, predicted = list(), list()\r\n","\r\n","with open('/content/drive/My Drive/files/NoCapital_Char_prediction.txt' , 'r', encoding='windows-1256') as predicted_file:\r\n","  for lines in predicted_file.readlines():\r\n","    lines = re.sub(r'\\s([^\\s])', r\"\\1\" , lines) #normalize the output\r\n","    predicted.append(lines.split())\r\n","  \r\n","with open('/content/drive/My Drive/files/unnorm.txt' , 'r', encoding='windows-1256') as actual_file:\r\n","  for lines in actual_file.readlines():\r\n","    lines = dataset_creator.preprocess_sentence(lines, \"t\")  #preprocess it so that the punctunation will one separate token\r\n","    lines = lines.replace(' |','')\r\n","    lines = lines.replace('^ ','')\r\n","    actual.append(lines.split())\r\n","\r\n","# calculate BLEU score\r\n","print('BLEU: %f' % corpus_bleu(actual, predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU: 0.478652\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]}]}